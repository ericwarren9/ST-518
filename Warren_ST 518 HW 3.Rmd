---
title: "ST 518 Homework 3"
author: "Eric Warren"
date: "September 15, 2023"
urlcolor: blue
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

```{r output setup, eval=FALSE, echo=FALSE}
# This code allows us to render a pdf document
rmarkdown::render("~/ST-518/Warren_ST 518 HW 3.Rmd", 
              output_format = "pdf_document", 
              output_options = list(
                toc = TRUE, 
                toc_depth = 3,
                number_sections = TRUE,
                df_print = "tibble"
                )
              )
```

# Problem 1

## Part A

Calculate the following matrix products.

- $X$
- $X'X$
- $X'Y$

### Part i

First we are going to calculate $X$. As we know this matrix is a ($n$ by $(p+1)$). Thus since $n = 4$ and $p = 2$, we have the $X$ matrix being a ($4$ by $(2+1)$) or ($4$ by $3$) matrix. We also know that the rows of this matrix are the observations with the columns having the first one just be the value of 1, the second one the value of the first predictor for the observation, and the third column being thee value of the second predictor for the observation. By looking at our chart we know that the $X$ matrix is $X = \begin{pmatrix} 1 & 1 & -2 \\ 1 & 0 & -1 \\ 1 & 0 & 2 \\ 1 & 2 & 1 \end{pmatrix}$.

### Part ii

Now we are going to calculate $X'X$. In this case we know what $X$ is from **Part i**, so now we are going to find $X'$ from *transposing* $X$. This is just moving the $i^{th}$ rows in $X$ to the $j^{th}$ columns in $X'$ and vice vsera. Thus by doing this flip, we can easily get the $X' = \begin{pmatrix} 1 & 1 & 1 & 1 \\ 1 & 0 & 0 & 2 \\ -2 & -1 & 2 & 1 \end{pmatrix}$. Now we need to find $X'X$. This is done by summing all the products of the $i^{th}$ row in $X'$ by the $j^{th}$ column in $X$. For example to find $X'X_{11}$ we would multiply the first element in the first row of the $X'$ matrix by the first element in the first column in $X$ and add that to the second element in the first row of the $X'$ matrix by the second element in the first column in $X$ and continue to add and do that multiplication until we get to the last element in the first row of the $X'$ matrix and multiply by the last element in the first column in $X$. This completes the first row and the first column of $X'X$ and we continue to do that until completing the matrix. Since we know that $X'X$ is a ($p+1$ by $p+1$) matrix and p+1 = 3, we should get a (3 x 3) matrix for $X'X$.

By doing our matrix multiplication we can see that $X'X$
$$= \begin{pmatrix} 1*1 + 1*1 + 1*1 + 1*1 & 1*1 + 1*0 + 1*0 + 1*2 & 1*-2 + 1*-1 + 1*2 + 1*1 \\ 1*1 + 0*1 + 0*1 + 2*1 & 1*1 + 0*0 + 0*0 + 2*2 & 1*-2 + 0*-1 + 0*2 + 2*1 \\ -2*1 + -1*1 + 2*1 + 1*1 & -2*1 + -1*0 + 2*0 + 1*2 & -2*-2 + -1*-1 + 2*2 + 1*1 \end{pmatrix}$$
$$= \begin{pmatrix} 1+1+1+1 & 1+0+0+2 & -2-1+2+1 \\ 1+0+0+2 & 1+0+0+4 & -2+0+0+2 \\ -2-1+2+1 & -2+0+0+2 & 4+1+1+4 \end{pmatrix}$$
$$= \begin{pmatrix} 4 & 3 & 0 \\ 3 & 5 & 0 \\ 0 & 0 & 10 \end{pmatrix}$$

Therefore, we have found that $X'X = \begin{pmatrix} 4 & 3 & 0 \\ 3 & 5 & 0 \\ 0 & 0 & 10 \end{pmatrix}$.

### Part iii

Now we are going to calculate $X'Y$. We know the $Y$ matrix is a ($n$ x 1) matrix and since $n$ = 4, we know that $Y$ is a (4 x 1) matrix. To find the $Y$ matrix, all we will do is put in the values from the *$y_i$* column in order. Therefore we can see that our $Y$ matrix is $Y = \begin{pmatrix} 68 \\ 57 \\ 90 \\ 112 \end{pmatrix}$. Now we will use the same type of matrix multiplication as we did for $X'X$ to get $X'Y$. In this case we will only have to worry about one column. We also know that the $X'Y$ matrix should be a ($p+1$ x 1) matrix form. Since $p+1$ = 3, then $X'Y$ is a (3 x 1) matrix. Now to solve $X'Y$
$$= \begin{pmatrix} 1*68 + 1*57 + 1*90 + 1*112 \\ 1*68 + 0*57 + 0*90 + 2*112 \\ -2*68 + -1*57 + 2*90 + 1*112 \end{pmatrix}$$
$$= \begin{pmatrix} 68+57+90+112 \\ 68+0+0+224 \\ -136-57+180+112 \end{pmatrix}$$
$$= \begin{pmatrix} 327 \\ 292\\ 99 \end{pmatrix}$$

Therefore, $X'Y = \begin{pmatrix} 327 \\ 292 \\ 99 \end{pmatrix}$.

## Part B

We are now going to find $(X'X)^{-1}$. 

To find this, first we are going to augment $X'X$ with the identity matrix to get $\begin{pmatrix} 4 & 3 & 0 &|& 1 & 0 & 0 \\ 3 & 5 & 0 &|& 0 & 1 & 0 \\ 0 & 0 & 10 &|& 0 & 0 & 1 \end{pmatrix}$. Now we are going to find the rows by trying to get the identity matrix on the left with our $(X'X)^{-1}$ on the right.

For purpose of notation we are going to do row numbers as $R_i$ where i is the row number and $R_i'$ as i being the row number and the **"'"** being the new matrix we should get to get to  $(X'X)^{-1}$ eventually being on the right and the identity matrix on the left. 

To start off with this, we are going to get the identity matrix on the last row by doing $R_3' = R_3 / 10$. This now gets us the matrix $\begin{pmatrix} 4 & 3 & 0 &|& 1 & 0 & 0 \\ 3 & 5 & 0 &|& 0 & 1 & 0 \\ 0 & 0 & 1 &|& 0 & 0 & \frac{1}{10} \end{pmatrix}$.

Now we are going to start getting the second row in order by first doing $R_2' = 3R_1 - 4R_2$. This will now get us the matrix of $\begin{pmatrix} 4 & 3 & 0 &|& 1 & 0 & 0 \\ 0 & -11 & 0 &|& 3 & -4 & 0 \\ 0 & 0 & 1 &|& 0 & 0 & \frac{1}{10} \end{pmatrix}$. To complete the second row, we need to get the second element in the row to be 1 by doing $R_2' = \frac{R_2'}{-11}$. Thus, this new matrix will now give us $\begin{pmatrix} 4 & 3 & 0 &|& 1 & 0 & 0 \\ 0 & 1 & 0 &|& \frac{-3}{11} & \frac{4}{11} & 0 \\ 0 & 0 & 1 &|& 0 & 0 & \frac{1}{10} \end{pmatrix}$.

Lastly, we need to get the first row in order by first doing $R_1' = R_1 - 3R_2$. This now gets us the matrix $\begin{pmatrix} 4 & 0 & 0 &|& \frac{20}{11} & \frac{-12}{11} & 0 \\ 0 & 1 & 0 &|& \frac{-3}{11} & \frac{4}{11} & 0 \\ 0 & 0 & 1 &|& 0 & 0 & \frac{1}{10} \end{pmatrix}$. To complete the first row, we need to get the first element in the row to equal 1 which we will do by saying $R_1' = \frac{R_1'}{4}$. Thus, our new matrix is now $\begin{pmatrix} 1 & 0 & 0 &|& \frac{5}{11} & \frac{-3}{11} & 0 \\ 0 & 1 & 0 &|& \frac{-3}{11} & \frac{4}{11} & 0 \\ 0 & 0 & 1 &|& 0 & 0 & \frac{1}{10} \end{pmatrix}$.

Since our identity matrix is on the left and completed, we know that $(X'X)^{-1}$ is on the right side. Therefore, we have found $(X'X)^{-1} = \begin{pmatrix} \frac{5}{11} & \frac{-3}{11} & 0 \\ \frac{-3}{11} & \frac{4}{11} & 0 \\ 0 & 0 & \frac{1}{10} \end{pmatrix}$. 

## Part C

We are going to obtain the product of $(X'X)^{-1}X'Y$. From **Part B**, we can see that $(X'X)^{-1} = \begin{pmatrix} \frac{5}{11} & \frac{-3}{11} & 0 \\ \frac{-3}{11} & \frac{4}{11} & 0 \\ 0 & 0 & \frac{1}{10} \end{pmatrix}$ and from **Part A, Part iii**, we can see that $X'Y = \begin{pmatrix} 327 \\ 292 \\ 99 \end{pmatrix}$. Since $(X'X)^{-1}$ is a (3 x 3) matrix and $X'Y$ is a (3 x 1) matrix, we know that $(X'X)^{-1}X'Y$ is going to be a (3 x 1) matrix as well (this is know since you take the row value of the first matrix and the column value of the second matrix if the column value of the first matrix matches up to the row value of the second matrix). Now we can solve $(X'X)^{-1}X'Y$
$$= \begin{pmatrix} \frac{5}{11} & \frac{-3}{11} & 0 \\ \frac{-3}{11} & \frac{4}{11} & 0 \\ 0 & 0 & \frac{1}{10} \end{pmatrix} * \begin{pmatrix} 327 \\ 292 \\ 99 \end{pmatrix}$$
$$= \begin{pmatrix} \frac{5}{11}*327 + \frac{-3}{11}*292 + 0*99 \\ \frac{-3}{11}*327 + \frac{4}{11}*292 + 0*99 \\ 0*327 + 0*292 + \frac{1}{10}*99 \end{pmatrix}$$

$$= \begin{pmatrix} \frac{1635}{11} - \frac{876}{11} + 0 \\ \frac{-981}{11} + \frac{1168}{11} + 0 \\ 0 + 0 + \frac{99}{10} \end{pmatrix}$$
$$= \begin{pmatrix} \frac{759}{11} \\ \frac{187}{11} \\ \frac{99}{10} \end{pmatrix}$$
$$= \begin{pmatrix} 69 \\ 17 \\ \frac{99}{10} \end{pmatrix}$$.

Therefore, we can see that $(X'X)^{-1}X'Y = \begin{pmatrix} 69 \\ 17 \\ \frac{99}{10} \end{pmatrix}$. We call this vector the **vector of regression parameters** which the first element gives us our intercept, the second element gives us our first predictor's partial slope, and the third element gives us the partial slope for the second predictor.

# Problem 2

Load in the `trees` data set and the `tidyverse` and `ppcor` packages. This will be needed to solve some questions about this data. We are then going to find an additive model of `Volume` on `Girth` and `Height`.
```{r}
# Load in packages
library(tidyverse)
library(ppcor)

# Load in data
trees <- as_tibble(trees)

# Make the additive model
treeVolumeAdditiveModel <- lm(Volume ~ Girth + Height, trees)
treeVolumeAdditiveModel
```

For **Parts A and B**, we are going to use the `pcor()` function to find these partial correlation coefficient values. Here we are going to show the table below and these will be used to answer our questions.
```{r}
pcor(trees)
```

## Part A

Here we are going to want to obtain the partial correlation coefficient between `Volume` and `Girth` after adjusting for linear dependence on `Height`. As we can see from our table above showing the partial correlation coefficient values, we can see that the partial correlation coefficient between `Volume` and `Girth` after adjusting for linear dependence on `Height` is `r pcor(trees)$estimate[3]`. We can find this value by looking at the `$estimate` part of the table and then finding where `Volume` and `Girth` match up in the table. That is how we find our value to be `r pcor(trees)$estimate[3]`.

## Part B

Here we are going to want to obtain the partial correlation coefficient between `Volume` and `Height` after adjusting for linear dependence on `Girth`. As we can see from our table above showing the partial correlation coefficient values, we can see that the partial correlation coefficient between `Volume` and `Height` after adjusting for linear dependence on `Girth` is `r pcor(trees)$estimate[2]`. We can find this value by looking at the `$estimate` part of the table and then finding where `Volume` and `Height` match up in the table. That is how we find our value to be `r pcor(trees)$estimate[2]`.

## Part C

We are going to try to find which two models the partial correlation coefficient is calculated as the correlation from in **Part A**. We know that it is defined by the correlation coefficient between the residuals computed from the two regressions below:

- $Y = \beta_0 + \beta_2*x_2 + ... + \beta_p * x_p$
- $X_1 = \beta_0 + \beta_2*x_2 + ... + \beta_p * x_p$

Since we know that the variables in question are `Volume` (which is our $Y$ variable) and `Girth` (our $X_1$ variable), we can plug that in for our models in this case with only two predictors. Thus the two models we are comparing which is used for our calculation are:

- $Y = \beta_0 + \beta_2*x_2$ which in words is saying `Volume` = $\beta_0 + beta_2$ * `Height`
- $X_1 = \beta_0 + \beta_2*x_2$ which in words is saying `Girth` = $\beta_0 + beta_2$ * `Height`

We will then get the residuals from both models and find the partial correlation by finding the correlation between $e_{y*2}$ and $e_{1*2}$.

# Problem 3

We know that there are 2 groups for supplements and $n = 20$ students who are doing a test.

## Part A

Consider a t-test comparing posttest scores ($y$) with and without the supplement without including pretest scores ($z$) in the analysis. Report the absolute value of the t-statistic and associated degrees of freedom from such a test.

Here we are trying to find the appropriate t-statistic to perform our test. What we should do is create an ANOVA table of the simple linear regression case and use this to help find our t-statistic. We know that our total degrees of freedom is $n-1 = 20-1 = 19$. We also know that our total sums of squares is 2166.2 from our chart. We can also find our `supp` degrees of freedom to be equal to $p = 1$ since it is one predictor ($p$) and thus our `Error` degrees of freedom is $n-p-1 = 20-1-1 = 18$. 

We also know that our Sum of Squares for `supp` is equal to the Type I SS in our chart for `supp`. As we can see this is equal to 24.2. Our Mean Square value for `supp` is equal to the sum of squares for `supp` divided by the degrees of freedom for `supp` which is equal to $\frac{24.2}{1} = 24.2$. 

Now for our sum of squares for `Error`. This is equal to the Total sum of squares minus the sum of squares for `supp` which is equal to $2166.2-24.2 = $`r 2166.2-24.2`. Our mean square error is just taking `r 2166.2-24.2` (or the sum of squares error value) and dividing it by the degrees of freedom for `Error`. Therefore we can see that our mean squared error is equal to $\frac{2166.2-24.2}{18} =$ `r (2166.2-24.2) / 18`. 

Lastly we need to get our F-value and p-value to complete the ANOVA table. Our F-value is just the mean squared value of `supp` divided by the mean square `Error`. Therefore we can see that the F-value is equal to $F = \frac{24.2}{\frac{2166.2-24.2}{18}} =$ `r 24.2 / ((2166.2-24.2) / 18)`. To find our p-value we are going to use the F-statistic value of `r 24.2 / ((2166.2-24.2) / 18)`, with the numerator degrees of freedom being 1 (the number of predictors) and the denominator degrees of freedom being 18 (the `Error` degrees of freedom). Therefore, we can find the p-value by using the `pf()` function in R by plugging in `pf(24.2 / ((2166.2-24.2) / 18), 1, 18, lower.tail = FALSE)`, which gives us the p-value of `r pf(24.2 / ((2166.2-24.2) / 18), 1, 18, lower.tail = FALSE)`. Now we have completed our ANOVA table for the simple linear regression case for post-test score (`y`) being modeled by if the supplement was given (`supp`). The ANOVA table is below.

|Source | DF | SS | MS | F-value | p-value
|----|:-----:|:-----:|:-----:|:------:|-----:|
|supp | 1 | 24.2 | 24.2 | `r 24.2 / ((2166.2-24.2) / 18)` |  `r ifelse(pf(24.2 / ((2166.2-24.2) / 18), 1, 18, lower.tail = FALSE) < .0001, "<.0001", pf(24.2 / ((2166.2-24.2) / 18), 1, 18, lower.tail = FALSE))`|
|Error | 18 | `r 2166.2-24.2` | `r (2166.2-24.2) / 18` | | |
|Corrected Total | 19 | 2166.2 | | | |

Now we were asked to find the t-statistic with its degrees of freedom to do the test. The degrees of freedom in a t-statistic is the same as the denominator degrees of freedom for the F-test. Therefore, we know the degrees of freedom for the t-statistic is 18. We also know that the absolute t-statistic for a two sided test is just the positive square root of the F-statistic ($t = \sqrt{F}$). So we take the square root of our f-statistic value of `r 24.2 / ((2166.2-24.2) / 18)` to get `r sqrt(24.2 / ((2166.2-24.2) / 18))`. We can check to make sure we get the same p-value with the t-statistic from the ANOVA table in which we do which is checked by `2 * pt(sqrt(24.2 / ((2166.2-24.2) / 18)), 18, lower.tail = FALSE)` which gives us this same p-value of `r 2 * pt(sqrt(24.2 / ((2166.2-24.2) / 18)), 18, lower.tail = FALSE)`. 

Therefore, we have found the absolute t-statistic value of `r sqrt(24.2 / ((2166.2-24.2) / 18))`, which has 18 degrees of freedom in the test. Lastly, we can say that if this test was conducted, we would fail to reject the $H_0$, as there is not statistically significant evidence to prove that `supp` is a needed predictor in our model (or its slope is not zero). 

## Part B

Consider a simple linear regression of posttest scores on pretest scores (z). Construct the ANOVA table from such a model. Report the coefficient of determination.

Here we can make the ANOVA table by looking at the output. We know that our only predictor is `z` in this case. So our degrees of freedom is just 1 as our only predictor in the model. Thus, the rest of our model comes from `Error` which has our degrees of freedom as $n-p-1 = n-1-1 = 20-1-1 = 18$. Our sum of squares for `z` in the output is 2015.30067 (which can be found by looking at the Type I and Type II SS categories). We know our total sum of squares by looking at the output is 2166.20000 so our `Error` sum of squares is the total of 2166.20000 minus the `z` sum of squares which is 2015.30067 which gives us an `Error` sum of squares of `r 2166.2-2015.30067`. Now our Mean Squared value for is just the sum of squares divided by its corresponding degrees of freedom. Thus, for the Mean Square of `z`, we can show that the sum of squares divided by degrees of freedom is just 2015.30067 / 1 = 2015.30067. For the Mean Square of `Error` we would show that the sum of squares divided by degrees of freedom, which is just `r 2166.2-2015.30067` / 18 = `r (2166.2-2015.30067) / 18`. We can find our F-value in our ANOVA table by taking the mean square value of `z` dividing it by the mean square value of `Error`, which gives us 2015.30067 / `r (2166.2-2015.30067) / 18` in which our F-value is `r 2015.30067 / ((2166.2-2015.30067) / 18)`. Lastly we can get our p-value by taking our F-value with 1 degree of freedom in the numerator (the model or `z` degrees of freedom) and 18 degrees of freedom in the denominator (the `Error` degrees of freedom). Limiting ourselves to four decimal points, we can find our p-value of our F-value which is F(`r 2015.30067 / ((2166.2-2015.30067) / 18)`, 1, 18) by using the `pf()` function in R and plugging in the values into it of pf(`r 2015.30067 / ((2166.2-2015.30067) / 18)`, 1, 18, lower.tail = FALSE) to get a p-value of `r ifelse(pf(2015.30067 / ((2166.2-2015.30067) / 18), 1, 18, lower.tail = FALSE) < .0001, "<.0001", pf(2015.30067 / ((2166.2-2015.30067) / 18), 1, 18, lower.tail = FALSE))`. Now we have all the values, we can fill in our ANOVA table.

|Source | DF | SS | MS | F-value | p-value
|----|:-----:|:-----:|:-----:|:------:|-----:|
|z | 1 | 2015.30067 | 2015.30067 | `r 2015.30067 / ((2166.2-2015.30067) / 18)` |  `r ifelse(pf(2015.30067 / ((2166.2-2015.30067) / 18), 1, 18, lower.tail = FALSE) < .0001, "<.0001", pf(2015.30067 / ((2166.2-2015.30067) / 18), 1, 18, lower.tail = FALSE))`|
|Error | 18 | `r 2166.2-2015.30067` | `r (2166.2-2015.30067) / 18` | | |
|Corrected Total | 19 | 2166.20000 | | | |

Now we are going to find the $R^2$ values. We can see from our ANOVA table where we can say that MS(R) = MS(`z`) and the MS(Tot) = MS(`Corrected Total`). Thus, we can get the $R^2$ by saying that $R^2 = \frac{MS(R)}{MS(Tot)} = \frac{2015.30067}{2166.20000} =$ `r 2015.30067 / 2166.20000`. We can see that our $R^2$ value is equal to `r 2015.30067 / 2166.20000`.

# Problem 4

Here we are going to load in the data and see how it looks.
```{r}
corn_data <- read_csv("cornwyear.csv")
corn_data
```

## Part A

Here we are going to load in each model and make the least squares regression equation by getting the coefficients.

### Part i -- Model 1

Our first model is $E(Y|x_1) = \beta_0 + \beta_1 * x_1$. We can find the coefficients below.
```{r}
model1_corn <- lm(yield ~ rain, corn_data)
summary(model1_corn)
```

From our output we can find the intercept ($\beta_0$) and our slope ($\beta_1$). Thus our regression line is $E(Y|x_1) =$ `r model1_corn$coefficients[[1]]` + `r model1_corn$coefficients[[2]]` * $x_1$.

### Part ii -- Model 2

Our second model is $E(Y|x_1, x_2) = \beta_0 + \beta_1 * x_1 + \beta_2 * x_2$. We can find the coefficients below. Note $x_2 = x_1^2$
```{r}
model2_corn <- lm(yield ~ rain + rain2, corn_data)
summary(model2_corn)
```

From our output we can find the intercept ($\beta_0$) and our partial slopes ($\beta_1$ and $\beta_2$). Thus our regression line is $E(Y|x_1) =$ `r model3_corn$coefficients[[1]]` + `r model3_corn$coefficients[[2]]` * $x_1$ + `r model3_corn$coefficients[[3]]` * $x_2$.

### Part iii -- Model 3

Our third model is $E(Y|x_1, x_3) = \beta_0 + \beta_1 * x_1 + \beta_3 * x_3$. We can find the coefficients below.
```{r}
model3_corn <- lm(yield ~ rain + year, corn_data)
summary(model3_corn)
```

From our output we can find the intercept ($\beta_0$) and our partial slopes ($\beta_1$ and $\beta_3$). Thus our regression line is $E(Y|x_1) =$ `r model3_corn$coefficients[[1]]` + `r model3_corn$coefficients[[2]]` * $x_1$ + `r model3_corn$coefficients[[3]]` * $x_3$.

### Part iv -- Model 4

Our fourth model is $E(Y|x_1, x_2, x_3) = \beta_0 + \beta_1 * x_1 + \beta_2 * x_2 + \beta_3 * x_3 + \beta_4 * x_1 * x_3 + \beta_5 * x_2 * x_3$. We can find the coefficients below.
```{r}
model4_corn <- lm(yield ~ rain + rain2 + year + rain_year + rain2_year, corn_data)
summary(model4_corn)
```

From our output we can find the intercept ($\beta_0$) and our partial slopes ($\beta_1$, $\beta_2$, $\beta_3$, $\beta_4$, and $\beta_5$). Thus our regression line is $E(Y|x_1) =$ `r model4_corn$coefficients[[1]]` + `r model4_corn$coefficients[[2]]` * $x_1$ + `r model4_corn$coefficients[[3]]` * $x_2$ + `r model4_corn$coefficients[[4]]` * $x_3$ + `r model4_corn$coefficients[[5]]` * $x_1 * x_3$ + `r model4_corn$coefficients[[6]]` * $x_2 * x_3$.

### Part v -- Model 5

Our fifth model is $E(Y|x_1, x_2, x_3) = \beta_0 + \beta_1 * x_1 + \beta_4 * x_1 * x_3 + \beta_5 * x_2 * x_3$. We can find the coefficients below.
```{r}
model5_corn <- lm(yield ~ rain + rain_year + rain2_year, corn_data)
summary(model5_corn)
```

From our output we can find the intercept ($\beta_0$) and our partial slopes ($\beta_1$, $\beta_4$, and $\beta_5$). Thus our regression line is $E(Y|x_1) =$ `r model5_corn$coefficients[[1]]` + `r model5_corn$coefficients[[2]]` * $x_1$ + `r model5_corn$coefficients[[3]]` * $x_1 * x_3$ + `r model5_corn$coefficients[[4]]` * $x_2 * x_3$.

### Part iv -- Model 6

Our sixth and final model is $E(Y|x_1, x_3) = \beta_0 + \beta_1 * x_1 +\beta_3 * x_3 + \beta_4 * x_1 * x_3$. We can find the coefficients below.
```{r}
model6_corn <- lm(yield ~ rain + year + rain_year, corn_data)
summary(model6_corn)
```

From our output we can find the intercept ($\beta_0$) and our partial slopes ($\beta_1$, $\beta_3$, and $\beta_4$). Thus our regression line is $E(Y|x_1) =$ `r model6_corn$coefficients[[1]]` + `r model6_corn$coefficients[[2]]` * $x_1$ + `r model6_corn$coefficients[[3]]` * $x_3$ + `r model6_corn$coefficients[[4]]` * $x_1 * x_3$.

## Part B

Conduct F-tests comparing the following pairs of models. For each comparison, state the implicit null hypothesis ($H_0$) being tested and conduct the test at level $\alpha = .05$. Additionally, report the p-value associated with the test/comparison. Using a policy that adopts the reduced/nested model unless there is “significant” evidence against $H_0$, specify the model you’d choose for each comparison.

### Part i -- Model 1 vs. Model 2

Here we are testing to see if we should add the quadratic term of rainfall into our model. In this case, we can say that $H_0: \beta_2 = 0$ with the $H_A: \beta_2 \neq 0$. We are going to complete an ANOVA test below to see if we should keep this variable.
```{r}
alpha <- 0.05
anova(model1_corn, model2_corn)
```

As we can see, our p-value is `r anova(model1_corn, model2_corn)$"Pr(>F)"[[2]]`. This is `r ifelse(anova(model1_corn, model2_corn)$"Pr(>F)"[[2]] < alpha, "less than", "greater than")` our alpha level at 0.05. Therefore, we should `r ifelse(anova(model1_corn, model2_corn)$"Pr(>F)"[[2]] < alpha, "reject", "fail to reject")` our null hypothesis as we  `r ifelse(anova(model1_corn, model2_corn)$"Pr(>F)"[[2]] < alpha, "have", "do not have")` statistically significant evidence to conclude that $\beta_2 \neq 0$. Therefore, we should use  `r ifelse(anova(model1_corn, model2_corn)$"Pr(>F)"[[2]] < alpha, "Model 2", "Model 1")` when doing our analysis due to $\beta_2$  `r ifelse(anova(model1_corn, model2_corn)$"Pr(>F)"[[2]] < alpha, "having", "not having")` an effect on our model.

### Part ii -- Model 3 vs. Model 4

Here we are testing to see if we should add the quadratic term of rainfall into our model. In this case, we can say that $H_0: \beta_2 = \beta_4 = \beta_5= 0$ with the $H_A: \beta_2 \neq \beta_4 \neq \beta_5 \neq 0$. We are going to complete an ANOVA test below to see if we should keep these variables.
```{r}
alpha <- 0.05
anova(model3_corn, model4_corn)
```

As we can see, our p-value is `r anova(model3_corn, model4_corn)$"Pr(>F)"[[2]]`. This is `r ifelse(anova(model3_corn, model4_corn)$"Pr(>F)"[[2]] < alpha, "less than", "greater than")` our alpha level at 0.05. Therefore, we should `r ifelse(anova(model3_corn, model4_corn)$"Pr(>F)"[[2]] < alpha, "reject", "fail to reject")` our null hypothesis as we  `r ifelse(anova(model3_corn, model4_corn)$"Pr(>F)"[[2]] < alpha, "have", "do not have")` statistically significant evidence to conclude that $\beta_2 \neq \beta_4 \neq \beta_5 \neq 0$. Therefore, we should use  `r ifelse(anova(model3_corn, model4_corn)$"Pr(>F)"[[2]] < alpha, "Model 4", "Model 3")` when doing our analysis due to $\beta_2, \beta_4,$ and/or $\beta_5$  `r ifelse(anova(model3_corn, model4_corn)$"Pr(>F)"[[2]] < alpha, "having", "not having")` an effect on our model.

### Part iii -- Model 4 vs. Model 5

Here we are testing to see if we should add the quadratic term of rainfall into our model. In this case, we can say that $H_0: \beta_2 = \beta_3 = 0$ with the $H_A: \beta_2 \neq \beta_3 \neq 0$. We are going to complete an ANOVA test below to see if we should keep these variables.
```{r}
alpha <- 0.05
anova(model4_corn, model5_corn)
```

As we can see, our p-value is `r anova(model4_corn, model5_corn)$"Pr(>F)"[[2]]`. This is `r ifelse(anova(model4_corn, model5_corn)$"Pr(>F)"[[2]] < alpha, "less than", "greater than")` our alpha level at 0.05. Therefore, we should `r ifelse(anova(model4_corn, model5_corn)$"Pr(>F)"[[2]] < alpha, "reject", "fail to reject")` our null hypothesis as we  `r ifelse(anova(model4_corn, model5_corn)$"Pr(>F)"[[2]] < alpha, "have", "do not have")` statistically significant evidence to conclude that $\beta_2 \neq \beta_3 \neq 0$. Therefore, we should use  `r ifelse(anova(model4_corn, model5_corn)$"Pr(>F)"[[2]] < alpha, "Model 5", "Model 4")` when doing our analysis due to $\beta_2$ and/or $\beta_3$  `r ifelse(anova(model4_corn, model5_corn)$"Pr(>F)"[[2]] < alpha, "having", "not having")` an effect on our model.

### Part iv -- Model 1 vs. Model 3

Here we are testing to see if we should add the quadratic term of rainfall into our model. In this case, we can say that $H_0: \beta_3 = 0$ with the $H_A: \beta_3 \neq 0$. We are going to complete an ANOVA test below to see if we should keep this variable.
```{r}
alpha <- 0.05
anova(model1_corn, model3_corn)
```

As we can see, our p-value is `r anova(model1_corn, model3_corn)$"Pr(>F)"[[2]]`. This is `r ifelse(anova(model1_corn, model3_corn)$"Pr(>F)"[[2]] < alpha, "less than", "greater than")` our alpha level at 0.05. Therefore, we should `r ifelse(anova(model1_corn, model3_corn)$"Pr(>F)"[[2]] < alpha, "reject", "fail to reject")` our null hypothesis as we  `r ifelse(anova(model1_corn, model3_corn)$"Pr(>F)"[[2]] < alpha, "have", "do not have")` statistically significant evidence to conclude that $\beta_3 \neq 0$. Therefore, we should use  `r ifelse(anova(model1_corn, model3_corn)$"Pr(>F)"[[2]] < alpha, "Model 3", "Model 1")` when doing our analysis due to $\beta_3$  `r ifelse(anova(model1_corn, model2_corn)$"Pr(>F)"[[2]] < alpha, "having", "not having")` an effect on our model.

### Part v -- Model 2 vs. Model 4

Here we are testing to see if we should add the quadratic term of rainfall into our model. In this case, we can say that $H_0: \beta_3 = \beta_4 = \beta_5= 0$ with the $H_A: \beta_3 \neq \beta_4 \neq \beta_5 \neq 0$. We are going to complete an ANOVA test below to see if we should keep these variables.
```{r}
alpha <- 0.05
anova(model2_corn, model4_corn)
```

As we can see, our p-value is `r anova(model2_corn, model4_corn)$"Pr(>F)"[[2]]`. This is `r ifelse(anova(model2_corn, model4_corn)$"Pr(>F)"[[2]] < alpha, "less than", "greater than")` our alpha level at 0.05. Therefore, we should `r ifelse(anova(model2_corn, model4_corn)$"Pr(>F)"[[2]] < alpha, "reject", "fail to reject")` our null hypothesis as we  `r ifelse(anova(model2_corn, model4_corn)$"Pr(>F)"[[2]] < alpha, "have", "do not have")` statistically significant evidence to conclude that $\beta_3 \neq \beta_4 \neq \beta_5 \neq 0$. Therefore, we should use  `r ifelse(anova(model2_corn, model4_corn)$"Pr(>F)"[[2]] < alpha, "Model 4", "Model 2")` when doing our analysis due to $\beta_3, \beta_4,$ and/or $\beta_5$  `r ifelse(anova(model3_corn, model4_corn)$"Pr(>F)"[[2]] < alpha, "having", "not having")` an effect on our model.

### Part vi -- Model 6 vs. Model 4

Here we are testing to see if we should add the quadratic term of rainfall into our model. In this case, we can say that $H_0: \beta_2 = \beta_5= 0$ with the $H_A: \beta_2 \neq \beta_5 \neq 0$. We are going to complete an ANOVA test below to see if we should keep these variables.
```{r}
alpha <- 0.05
anova(model6_corn, model4_corn)
```

As we can see, our p-value is `r anova(model6_corn, model4_corn)$"Pr(>F)"[[2]]`. This is `r ifelse(anova(model6_corn, model4_corn)$"Pr(>F)"[[2]] < alpha, "less than", "greater than")` our alpha level at 0.05. Therefore, we should `r ifelse(anova(model6_corn, model4_corn)$"Pr(>F)"[[2]] < alpha, "reject", "fail to reject")` our null hypothesis as we  `r ifelse(anova(model6_corn, model4_corn)$"Pr(>F)"[[2]] < alpha, "have", "do not have")` statistically significant evidence to conclude that $\beta_2 \neq \beta_5 \neq 0$. Therefore, we should use  `r ifelse(anova(model6_corn, model4_corn)$"Pr(>F)"[[2]] < alpha, "Model 4", "Model 6")` when doing our analysis due to $\beta_2$ and/or $\beta_5$  `r ifelse(anova(model6_corn, model4_corn)$"Pr(>F)"[[2]] < alpha, "having", "not having")` an effect on our model.

## Part C

We are looking to see if Model 2 is nested in Model 3. And while we can put the linear restriction of $\beta_2 = 0$ in Model 3, this only gets to $E(Y|x_1) = \beta_0 + \beta_1 * x_1$, which is Model 1 not Model 2. Model 2 has the extra $\beta_2 * x_1^2$ term that can not be obtained from Model 3, no matter what linear restrictions we place on it. This makes sense as well since Model 2 is a quadratic model while Model 3 is linear and we cannot nest a quadratic model within a linear model. For these reasons, Model 2 is not nested within Model 3.

## Part D

Use Model 1 and Model 6 to separately estimate the increase in yield when rainfall increases by 1 inch in the year 1900.

- Model 1: Here we know that $E(Y|x_1) = \beta_0 + \beta_1 * x_1$ where $x_1$ is rainfall. We know that $\beta_0$ remains constant, so we are only looking at $\beta_1$ which is telling us that we expect the yield to increase by $\beta_1$ amount per one increase in units for $x_1$ (which in this case is one inch of rainfall). Thus, our estimated increase in yield in 1900 for each increase of inch of rainfall is just $\beta_1$ which is `r model1_corn$coefficients[[2]]`.
- Model 6: Here we know that $E(Y|x_1, x_3) = \beta_0 + \beta_1 * x_1 + \beta_3 * x_3 + \beta_4 * x_1 * x_3$, where $x_1$ is rainfall and $x_3$ is year. Since year is constant, we can plug in 1900 for each of the $x_3$'s to get our new model. Now we can see that $E(Y|x_1, x_3 = 1900) = \beta_0 + \beta_1 * x_1 +\beta_3 * 1900 + \beta_4 * x_1 * 1900 = (\beta_0 + 1900*\beta_3) + (\beta_1 + 1900 * \beta_4) * x_1$. The left side of the equation $(\beta_0 + 1900*\beta_3)$ is a constant so we only need to look at the right side of $(\beta_1 + 1900 * \beta_4) * x_1$ to find out what we would expect our estimated increase in yield in 1900 for each increase of inch in rainfall is just $(\beta_1 + 1900 * \beta_4)$ (where $\beta_1$ = `r model6_corn$coefficients[[2]]` and $\beta_4$ = `r model6_corn$coefficients[[2]]`) which is `r model6_corn$coefficients[[2]] + 1900 * model6_corn$coefficients[[4]]`. Therefore, we would expect our estimated increase in yield in 1900 for each increase of inch in rainfall is `r model6_corn$coefficients[[2]] + 1900 * model6_corn$coefficients[[4]]`.

## Part E

Consider Model 3. Is the model well-determined in the sense that both estimated regression coefficients differ significantly from 0? How much do you estimate that mean corn yield increases each year from 1890-1927, controlling for rain? Report the standard error for your estimate. Provide some explanation (“narrative”) as to why corn yields appear to be increasing over time even after controlling for rainfall.

First let us take a look at Model 3 again by looking at some of its summary statistics.
```{r}
summary(model3_corn)
```

Using the t-statistics for partial slopes we can see that both offer p-values less than 0.05, which means we have statistically significant evidence to say that both estimated regression coefficients differ significantly from 0. If we control for rain, we can look at our output to see that our partial slope for year is `r summary(model3_corn)$coefficients[3, 1]`, which means that we expect the mean corn yield to increase by `r summary(model3_corn)$coefficients[3, 1]` from 1890-1927, if we control for rainfall. We can also see that the standard error for `year` is `r summary(model3_corn)$coefficients[3, 2]`, so our standard error of our estimate here is `r summary(model3_corn)$coefficients[3, 2]`. 

Corn yield can increase over time, despite looking at rainfall for a number of reasons. First from a data standpoint, we can look at the following in which we can see that some of the lower outliers occur in earlier years. 
```{r}
plot(corn_data$year, corn_data$yield)
```
As we can see, there are some dips in the later 1800s to early 1900s. Since we are performing a line of best fit, we would assume that `year` would produce an increasing amount of yield as those select years have a large impact, given the smallish sample size. We can also see in later years that a couple have some spikes further making our model believe that year is a factor, despite most of the time `year` having a steady amount. Due to what has been described, this is why that `year` appears to cause an increasing corn yield, even after controlling the rainfall.